import streamlit as st
import feedparser
import time
from datetime import datetime
import json
import os
import concurrent.futures
import html
import urllib.parse
from typing import List, Dict, Tuple, Optional, Any

# ================= 1. Constants =================

# VIP å…¬å¸æ¸…å–® (å°å°¼é‡é»å°å•†)
# Includes user-added ASUS and Acer
VIP_COMPANIES_EN: List[str] = [
    '"Foxconn"', '"Hon Hai"', '"Pegatron"', '"Delta Electronics"', 
    '"Compal"', '"Gogoro"', '"Kymco"', '"Pou Chen"', 
    '"Eclat Textile"', '"Cheng Shin"', '"CTBC Bank"',
    '"ASUS"', '"Acer"'
]

VIP_COMPANIES_CN: List[str] = [
    'é´»æµ·', 'å¯Œå£«åº·', 'å’Œç¢©', 'å°é”é›»', 
    'ä»å¯¶', 'Gogoro', 'å…‰é™½', 'å¯¶æˆ', 
    'å„’é´»', 'æ­£æ–°', 'ä¸­ä¿¡éŠ€',
    'è¯ç¢©', 'å®ç¢'
]

# é å…ˆè¨ˆç®—å¥½æŸ¥è©¢å­—ä¸²
# Google News RSS strictness check: Use %20 for spaces
# Use logic from verified fix: quote() and %20OR%20
# Also user's version had wrapping parentheses, which is good for boolean logic security.
VIP_QUERY_EN: str = "(" + "%20OR%20".join([urllib.parse.quote(c) for c in VIP_COMPANIES_EN]) + ")"
VIP_QUERY_CN: str = "(" + "%20OR%20".join([urllib.parse.quote(c) for c in VIP_COMPANIES_CN]) + ")"

# é¸é …æ˜ å°„
DATE_MAP: Dict[str, int] = {
    "3å¤©": 3, "1é€±": 7, "1æœˆ": 30
}

TOPIC_MAP: Dict[str, str] = {
    "å°å°¼æ”¿ç¶“": "macro",
    "é›»å‹•è»Šèˆ‡ä¾›æ‡‰éˆ": "industry",
    "é‡é»å°å•†": "vip"
}

# ================= 2. Helper Functions =================

def get_rss_sources(days: int, mode: str = "all", custom_keyword: Optional[str] = None) -> List[Dict[str, str]]:
    """
    ç”¢ç”Ÿ RSS ä¾†æºåˆ—è¡¨
    :param days: å¤©æ•¸ (int)
    :param mode: æœå°‹æ¨¡å¼ (custom, macro, industry, vip)
    :param custom_keyword: è‡ªè¨‚é—œéµå­—
    """
    sources = []
    
    # è‡ªè¨‚æœå°‹æ¨¡å¼
    if mode == "custom" and custom_keyword:
        # Improved URL encoding: use quote() for %20
        clean_keyword = custom_keyword.strip()
        encoded_keyword = urllib.parse.quote(clean_keyword)
        
        sources.append({
            "name": f"ğŸ” æ·±åº¦è¿½è¹¤: {clean_keyword} (ä¸­)",
            "url": f"https://news.google.com/rss/search?q={encoded_keyword}%20when:{days}d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
        })
        sources.append({
            "name": f"ğŸ” æ·±åº¦è¿½è¹¤: {clean_keyword} (EN)",
            "url": f"https://news.google.com/rss/search?q={encoded_keyword}%20when:{days}d&hl=en-ID&gl=ID&ceid=ID:en"
        })
        return sources

    # é è¨­æ¨¡å¼
    if mode == "macro":
        sources.extend([
            {"name": "ğŸ‡®ğŸ‡© å°å°¼æ•´é«” (ä¸­)", "url": f"https://news.google.com/rss/search?q={urllib.parse.quote('å°å°¼')}%20when:{days}d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"},
            {"name": "ğŸ‡®ğŸ‡© å°å°¼æ•´é«” (EN)", "url": f"https://news.google.com/rss/search?q={urllib.parse.quote('Indonesia')}%20when:{days}d&hl=en-ID&gl=ID&ceid=ID:en"},
            {"name": "ğŸ‡¹ğŸ‡¼ å°å°é—œä¿‚ (ä¸­)", "url": f"https://news.google.com/rss/search?q={urllib.parse.quote('å°å°¼ å°ç£ OR "å°å•†"')}%20when:{days}d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"},
            {"name": "ğŸ‡¹ğŸ‡¼ å°å°é—œä¿‚ (EN)", "url": f"https://news.google.com/rss/search?q={urllib.parse.quote('Indonesia Taiwan OR "Taiwanese investment"')}%20when:{days}d&hl=en-ID&gl=ID&ceid=ID:en"}
        ])
    elif mode == "industry":
        # å°å°¼é—œéµå­—ï¼šEV, Battery, Nickel, Electronics
        sources.extend([
            {"name": "âš¡ EV/é›»å­ (ä¸­)", "url": f"https://news.google.com/rss/search?q={urllib.parse.quote('å°å°¼ é›»å‹•è»Š OR é›»æ±  OR "é›»å­è£½é€ "')}%20when:{days}d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"},
            {"name": "âš¡ EV/Electronics (EN)", "url": f"https://news.google.com/rss/search?q={urllib.parse.quote('Indonesia EV OR Battery OR Nickel OR Electronics Manufacturing')}%20when:{days}d&hl=en-ID&gl=ID&ceid=ID:en"}
        ])
    elif mode == "vip":
        # ä½¿ç”¨å…¨åŸŸè®Šæ•¸ VIP_QUERY_CN/EN
        # æ³¨æ„: é€™è£¡çš„ URL å·²ç¶“ç”± VIP_QUERY_* è™•ç†å¥½ç·¨ç¢¼çš„ä¸€éƒ¨åˆ† (%20OR%20)
        # Optimized logic: (Indonesia) AND (Company A OR Company B...)
        sources.extend([
            {"name": "ğŸ¢ å°å•†å‹•æ…‹ (ä¸­)", "url": f"https://news.google.com/rss/search?q={urllib.parse.quote('å°å°¼')}%20{VIP_QUERY_CN}%20when:{days}d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"},
            {"name": "ğŸ¢ å°å•†å‹•æ…‹ (EN)", "url": f"https://news.google.com/rss/search?q={urllib.parse.quote('Indonesia')}%20{VIP_QUERY_EN}%20when:{days}d&hl=en-ID&gl=ID&ceid=ID:en"}
        ])
    
    return sources

def fetch_feed(source: Dict[str, str]) -> Tuple[Dict[str, str], Any]:
    """Helper function to fetch a single RSS feed."""
    try:
        # Best practice: Add User-Agent headers
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        # Note: feedparser allows passing headers or agent, but it's often better to fetch content with requests first
        # However, to keep it simple and consistent with previous working version, let's use feedparser's built-in http support first.
        # If google blocks it, we might need requests. But debug script showed logic.fetch_feed working with valid URL.
        # Let's keep it simple as in Step 22/115 but with correct URLs.
        return source, feedparser.parse(source['url'])
    except Exception as e:
        return source, None

def generate_chatgpt_prompt(days_label: str, days_int: int, search_mode: str, custom_keyword: Optional[str] = None) -> Tuple[str, List[Dict[str, str]]]:
    """
    åŸ·è¡Œçˆ¬èŸ²ä¸¦ç”Ÿæˆ Prompt
    """
    status_text = st.empty() 
    progress_bar = st.progress(0)
    
    # å‘¼å« get_rss_sourcesï¼Œä¸¦å‚³å…¥ days_int ä½œç‚º days åƒæ•¸
    sources = get_rss_sources(days_int, search_mode, custom_keyword)
    news_items_for_json = []

    if search_mode == "custom":
        instruction_prompt = f"é‡å°é—œéµå­—ã€{custom_keyword}ã€‘ï¼Œè«‹æ’°å¯«ä¸€ä»½æ·±åº¦åˆ†æå ±å‘Šï¼š1. é‡é»æ‘˜è¦ 2. å¸‚å ´å½±éŸ¿ 3. æ©Ÿæœƒèˆ‡é¢¨éšªã€‚"
    elif search_mode == "macro":
        instruction_prompt = f"è«‹åˆ†æã€{days_label} å°å°¼æ•´é«”èˆ‡å°å°é—œä¿‚ã€‘ï¼š1. å°å°¼æ”¿ç¶“å±€å‹¢ (å«æ–°é¦–éƒ½/æ”¿ç­–) 2. å°å°é›™é‚Šäº’å‹•ã€‚"
    elif search_mode == "industry":
        instruction_prompt = f"è«‹åˆ†æã€{days_label} å°å°¼é›»å‹•è»Šèˆ‡é›»å­ç”¢æ¥­ã€‘ï¼š1. ç”¢æ¥­è¶¨å‹¢ (EV/é›»æ± /é³ç¤¦) 2. ä¾›æ‡‰éˆå‹•æ…‹ã€‚"
    elif search_mode == "vip":
        instruction_prompt = f"è«‹åˆ†æã€{days_label} å°å°¼é‡é»å°å•†ã€‘ï¼š1. å€‹è‚¡å‹•æ…‹ 2. æŠ•è³‡è¨Šè™Ÿã€‚"
    else:
        instruction_prompt = "è«‹åˆ†æä»¥ä¸‹æ–°èã€‚"

    output_text = f"""
è«‹æ‰®æ¼”ä¸€ä½è³‡æ·±çš„ã€Œæ±å—äºç”¢æ¥­åˆ†æå¸«ã€ã€‚
{instruction_prompt}
è«‹ç”¨**ç¹é«”ä¸­æ–‡**ï¼Œä¸¦ä»¥ **Markdown** æ¢åˆ—å¼è¼¸å‡ºï¼Œé¢¨æ ¼éœ€å°ˆæ¥­ä¸”æ˜“è®€ã€‚

========= ä»¥ä¸‹æ˜¯æ–°èè³‡æ–™åº« ({datetime.now().strftime('%Y-%m-%d')}) =========
"""
    
    seen_titles = set()
    total_steps = len(sources)
    
    # å¹³è¡ŒæŠ“å– RSS
    status_text.text(f"ğŸ“¡ æ­£åœ¨å¹³è¡Œæƒæ {len(sources)} å€‹ä¾†æº...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_source = {executor.submit(fetch_feed, source): source for source in sources}
        
        completed_count = 0
        for future in concurrent.futures.as_completed(future_to_source):
            completed_count += 1
            progress_bar.progress(completed_count / total_steps)
            
            source, feed = future.result()
            
            if feed and len(feed.entries) > 0:
                output_text += f"\\n## ã€{source['name']}ã€‘\\n"
                
                # è‡ªè¨‚æœå°‹ä¸è¨­é™ï¼Œé è¨­é™åˆ¶ 30 ç¯‡
                limit = len(feed.entries) if search_mode == "custom" else 30
                
                for entry in feed.entries[:limit]: 
                    if entry.title in seen_titles: continue
                    seen_titles.add(entry.title)
                    source_name = entry.source.title if 'source' in entry else "Google News"
                    pub_date = entry.published if 'published' in entry else ""
                    link = entry.link
                    
                    output_text += f"- [{pub_date}] [{source_name}] {entry.title}\\n  é€£çµ: {link}\\n"
                    news_items_for_json.append({
                        "title": entry.title, "link": link, "date": pub_date,
                        "source": source_name, "category": source['name']
                    })
            else:
                output_text += f"\\n## ã€{source['name']}ã€‘\\n(ç„¡ç›¸é—œæ–°è)\\n"

    output_text += "\\n========= è³‡æ–™çµæŸ ========="
    
    # ç´¯ç©æ­·å²è³‡æ–™é‚è¼¯
    try:
        existing_data = {"news_list": []}
        if os.path.exists('news_data.json'):
            with open('news_data.json', 'r', encoding='utf-8') as f:
                try:
                    existing_data = json.load(f)
                except json.JSONDecodeError:
                    pass # æª”æ¡ˆææ¯€å‰‡ä½¿ç”¨ç©ºåˆ—è¡¨
        
        # å»ºç«‹ç¾æœ‰é€£çµé›†åˆä»¥éæ¿¾é‡è¤‡
        existing_list = existing_data.get('news_list', [])
        existing_links = set(item['link'] for item in existing_list)
        
        # Insert new items
        for item in news_items_for_json:
            if item['link'] not in existing_links:
                existing_list.insert(0, item) # æ–°çš„æ”¾å‰é¢
                existing_links.add(item['link'])
        
        # Optimization: Limit to latest 1000 items
        if len(existing_list) > 1000:
            existing_list = existing_list[:1000]
        
        existing_data['news_list'] = existing_list
        existing_data['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        with open('news_data.json', 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=4)
            
    except Exception as e:
        print(f"å­˜æª”å¤±æ•—: {e}")

    status_text.text("âœ… å®Œæˆï¼")
    time.sleep(0.5)
    status_text.empty()
    progress_bar.empty()
    
    return output_text, news_items_for_json